{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "import inception\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from detnet import det_net, det_net_loss\n",
    "import nnutil\n",
    "import volleyball\n",
    "from volleyball import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/videos\n"
     ]
    }
   ],
   "source": [
    "## config\n",
    "class Config(object):\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    # shared\n",
    "    self.image_size = 720, 1280\n",
    "    self.out_size = 87, 157\n",
    "    self.batch_size = 4\n",
    "    self.num_boxes = 12\n",
    "    self.epsilon = 1e-5\n",
    "    self.features_multiscale_names = ['Mixed_5d', 'Mixed_6e']\n",
    "    self.train_inception = False\n",
    "\n",
    "    # DetNet\n",
    "    self.build_detnet = False\n",
    "    self.num_resnet_blocks = 1\n",
    "    self.num_resnet_features = 512\n",
    "    self.reg_loss_weight = 10.0\n",
    "    self.nms_kind = 'greedy'\n",
    "\n",
    "    # ActNet\n",
    "    self.use_attention = False\n",
    "    self.crop_size = 5, 5\n",
    "    self.num_features_boxes = 4096\n",
    "    self.num_actions = 9\n",
    "    self.num_activities = 8\n",
    "    self.actions_loss_weight = 4.0\n",
    "    self.actions_weights = [[ 1., 1., 2., 3., 1., 2., 2., 0.2, 1.]]\n",
    "\n",
    "    # sequence\n",
    "    self.num_features_hidden = 1024\n",
    "    self.num_frames = 10\n",
    "    self.num_before = 5\n",
    "    self.num_after = 4\n",
    "\n",
    "    # training parameters\n",
    "    self.train_num_steps = 5000\n",
    "    self.train_random_seed = 0\n",
    "    self.train_learning_rate = 1e-5\n",
    "    self.train_dropout_prob = 0.8\n",
    "    self.train_save_every_steps = 500\n",
    "\n",
    "src_model_config = 'models/activity/volleyball/config-temporal-actions-2048-5x5-weight-0.5-match-hidden-soft.pkl'\n",
    "\n",
    "# we load config from the first stage\n",
    "c = pickle.load(open(src_model_config, 'rb'))\n",
    "c.src_model_path = c.out_model_path\n",
    "c.tag = c.tag.replace('single', 'temporal')\n",
    "# smaller batch size for the temporal\n",
    "c.batch_size = 3\n",
    "# not finetuning inception at training\n",
    "c.train_inception = False\n",
    "# don't need detection net during training\n",
    "c.build_detnet = False\n",
    "# 'hidden', 'hidden-soft', 'boxes-soft'\n",
    "c.match_kind = 'hidden-soft'\n",
    "c.tag += '-match-' + c.match_kind\n",
    "c.ckpt_dir = 'models/activity/volleyball/'\n",
    "c.data_path = '../Data/videos'\n",
    "c.images_path = '../Data/videos'\n",
    "print(c.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1337\n",
      "3493\n"
     ]
    }
   ],
   "source": [
    "## reading the dataset\n",
    "train = volley_read_dataset(c.data_path, TRAIN_SEQS + VAL_SEQS)\n",
    "train_frames = volley_all_frames(train)\n",
    "test = volley_read_dataset(c.data_path, TEST_SEQS)\n",
    "test_frames = volley_all_frames(test)\n",
    "\n",
    "all_anns = {**train, **test}\n",
    "all_tracks = pickle.load(open(c.data_path + '/tracks_normalized.pkl', 'rb')) #a dict of dicts containing the bboxes\n",
    "print(len(test_frames))\n",
    "print(len(train_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading utils\n",
    "def _frames_around(frame, num_before=5, num_after=4):\n",
    "  sid, src_fid = frame\n",
    "  return [(sid, src_fid, fid)\n",
    "          for fid in range(src_fid-num_before, src_fid+num_after+1)]\n",
    "\n",
    "def load_samples_sequence(anns, tracks, images_path, frames, num_boxes=12):\n",
    "  # NOTE: this assumes we got the same # of boxes for this batch\n",
    "  images, boxes, boxes_idx = [], [], []\n",
    "  activities, actions = [], []\n",
    "  for i, (sid, src_fid, fid) in enumerate(frames):\n",
    "    images.append(skimage.io.imread(images_path + '/%d/%d/%d.jpg' %\n",
    "                                    (sid, src_fid, fid)))\n",
    "\n",
    "    boxes.append(tracks[(sid, src_fid)][fid])\n",
    "    actions.append(anns[sid][src_fid]['actions'])\n",
    "    if len(boxes[-1]) != num_boxes:\n",
    "      boxes[-1] = np.vstack([boxes[-1], boxes[-1][:num_boxes-len(boxes[-1])]])\n",
    "      actions[-1] = actions[-1] + actions[-1][:num_boxes-len(actions[-1])]\n",
    "    boxes_idx.append(i * np.ones(num_boxes, dtype=np.int32))\n",
    "    activities.append(anns[sid][src_fid]['group_activity'])\n",
    "\n",
    "\n",
    "  images = np.stack(images)\n",
    "  activities = np.array(activities, dtype=np.int32)\n",
    "  bboxes = np.vstack(boxes).reshape([-1, num_boxes, 4])\n",
    "  bboxes_idx = np.hstack(boxes_idx).reshape([-1, num_boxes])\n",
    "  actions = np.hstack(actions).reshape([-1, num_boxes])\n",
    "\n",
    "  return images, activities, bboxes, bboxes_idx, actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished building graph\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "  H, W = c.image_size\n",
    "  OH, OW = c.out_size\n",
    "  B, T, N = c.batch_size, c.num_frames, c.num_boxes\n",
    "  NFB, NFH = c.num_features_boxes, c.num_features_hidden\n",
    "  EPS = c.epsilon\n",
    "\n",
    "  # each batch is a single (small) sequence\n",
    "  images_in = tf.placeholder(tf.uint8, [B,T,H,W,3], 'images_in')\n",
    "  boxes_in = tf.placeholder(tf.float32, [B,T,N,4], 'boxes_in')\n",
    "  boxes_idx_in = tf.placeholder(tf.int32, [B,T,N,], 'boxes_idx_in')\n",
    "  actions_in = tf.placeholder(tf.int32, [B,T,N,], 'actions_in')\n",
    "  activities_in = tf.placeholder(tf.int32, [B,T,], 'activities_in')\n",
    "  dropout_keep_prob_in = tf.placeholder(tf.float32, [], 'dropout_keep_prob_in')\n",
    "\n",
    "  images_in_flat = tf.reshape(images_in, [B*T,H,W,3])\n",
    "  boxes_in_flat = tf.reshape(boxes_in, [B*T*N,4])\n",
    "  boxes_idx_in_flat = tf.reshape(boxes_idx_in, [B*T*N,])\n",
    "  actions_in_flat = tf.reshape(actions_in, [B*T*N,])\n",
    "  activities_in_flat = tf.reshape(activities_in, [B*T,])\n",
    "\n",
    "  # TODO: only construct inception until a certain level\n",
    "  _, inception_endpoints = inception.inception_v3(inception.process_images(images_in_flat),\n",
    "                                                  trainable=c.train_inception,\n",
    "                                                  is_training=False,\n",
    "                                                  create_logits=False,\n",
    "                                                  scope='InceptionV3')\n",
    "  inception_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'InceptionV3')\n",
    "\n",
    "  # extracting multiscale features\n",
    "  features_multiscale = []\n",
    "  for name in c.features_multiscale_names:\n",
    "    features = inception_endpoints[name]\n",
    "    if features.get_shape()[1:3] != tf.TensorShape([OH, OW]):\n",
    "      features = tf.image.resize_images(features, [OH, OW])\n",
    "    features_multiscale.append(features)\n",
    "  features_multiscale = tf.concat(features_multiscale,3)\n",
    "\n",
    "  if c.build_detnet:\n",
    "    # TODO: instead of boxes_in\n",
    "    seg_preds, reg_preds, boxes_proposals, detections = det_net(features_multiscale,\n",
    "                                                                c.num_resnet_blocks,\n",
    "                                                                c.num_resnet_features,\n",
    "                                                                N,\n",
    "                                                                [H, W],\n",
    "                                                                c.nms_kind)\n",
    "    boxes_preds, boxes_confidence = detections\n",
    "    det_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'DetNet')\n",
    "\n",
    "\n",
    "  with tf.variable_scope('ActNet'):\n",
    "    boxes_flat = boxes_in_flat\n",
    "    boxes_idx_flat = boxes_idx_in_flat\n",
    "#     boxes_flat = tf.reshape(boxes_preds, [B*N,4])\n",
    "#     # TODO: double-check\n",
    "#     boxes_idx_flat = tf.tile(tf.range(0, B)[:,tf.newaxis], [1, N*T])\n",
    "    boxes_features_multiscale = tf.image.crop_and_resize(features_multiscale,\n",
    "                                                         boxes_flat,\n",
    "                                                         boxes_idx_flat,\n",
    "                                                         c.crop_size)\n",
    "    boxes_features_multiscale_flat = slim.flatten(boxes_features_multiscale)\n",
    "\n",
    "    with tf.variable_scope('shared'):\n",
    "      boxes_features_flat = slim.fully_connected(boxes_features_multiscale_flat, NFB)\n",
    "      boxes_features_flat_dropout = slim.dropout(boxes_features_flat, dropout_keep_prob_in)\n",
    "    shared_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'ActNet/shared')\n",
    "\n",
    "    with tf.variable_scope('sequence'):\n",
    "      # embedding to \"hidden space\"\n",
    "      boxes_hidden_flat = slim.fully_connected(boxes_features_flat_dropout, NFH, tf.nn.tanh)\n",
    "      boxes_hidden = tf.reshape(boxes_hidden_flat, [B,T,N,NFH])\n",
    "\n",
    "      def _construct_sequence(batch):\n",
    "        hidden, boxes = batch\n",
    "        # initializing the state with features\n",
    "        states = [hidden[0]]\n",
    "        # TODO: make this dependent on the data\n",
    "        # TODO: make it with scan ?\n",
    "        for t in range(1, T):\n",
    "          # find the matching boxes. TODO: try with the soft matching function\n",
    "          if c.match_kind == 'boxes':\n",
    "            dists = nnutil.cdist(boxes[t-1], boxes[t])\n",
    "            idxs = tf.argmin(dists, 1, 'idxs')\n",
    "            state_prev = tf.gather(states[t-1], idxs)\n",
    "          elif c.match_kind == 'hidden':\n",
    "            # TODO: actually it makes more sense to compare on states\n",
    "            dists = nnutil.cdist(hidden[t-1], hidden[t])\n",
    "            idxs = tf.argmin(dists, 1, 'idxs')\n",
    "            state_prev = tf.gather(states[t-1], idxs)\n",
    "          elif c.match_kind == 'hidden-soft':\n",
    "            dists = nnutil.cdist(hidden[t-1], hidden[t])\n",
    "            weights = slim.softmax(dists)\n",
    "            state_prev = tf.matmul(weights, states[t-1])\n",
    "          else:\n",
    "            raise RuntimeError('Unknown match_kind: %s' % c.match_kind)\n",
    "\n",
    "          def _construct_update(reuse):\n",
    "            state = tf.concat([state_prev, hidden[t]], 1)\n",
    "            # TODO: initialize jointly\n",
    "            reset = slim.fully_connected(state, NFH, tf.nn.sigmoid,\n",
    "                                         reuse=reuse,\n",
    "                                         scope='reset')\n",
    "            step = slim.fully_connected(state, NFH, tf.nn.sigmoid,\n",
    "                                        reuse=reuse,\n",
    "                                        scope='step')\n",
    "            state_r = tf.concat([reset * state_prev, hidden[t]], 1)\n",
    "            state_up = slim.fully_connected(state_r, NFH, tf.nn.tanh,\n",
    "                                            reuse=reuse,\n",
    "                                            scope='state_up')\n",
    "            return state_up, step\n",
    "          try:\n",
    "            state_up, step = _construct_update(reuse=True)\n",
    "          except ValueError:\n",
    "            state_up, step = _construct_update(reuse=False)\n",
    "\n",
    "          state = step * state_up + (1.0 - step) * state_prev\n",
    "          states.append(state)\n",
    "        return tf.stack(states)\n",
    "\n",
    "      boxes_states = tf.map_fn(_construct_sequence,\n",
    "                               [boxes_hidden, boxes_in],\n",
    "                               dtype=np.float32)\n",
    "\n",
    "      # prediction!\n",
    "      # for each of the states, we reuse the same weights\n",
    "      with tf.variable_scope('actions_hidden'):\n",
    "        boxes_states_flat = tf.reshape(boxes_states, [-1, NFH])\n",
    "        actions_logits = slim.fully_connected(boxes_states_flat,\n",
    "                                              c.num_actions,\n",
    "                                              None)\n",
    "        actions_preds = slim.softmax(actions_logits, 'preds')\n",
    "        actions_in_one_hot = slim.one_hot_encoding(actions_in_flat, c.num_actions)\n",
    "        actions_loss = - tf.reduce_mean(tf.constant(c.actions_weights) *\n",
    "                                        actions_in_one_hot * tf.log(actions_preds + EPS))\n",
    "        actions_labels = tf.argmax(actions_logits, 1)\n",
    "        actions_accuracy = tf.reduce_mean(tf.to_float(tf.equal(tf.to_int32(actions_labels),\n",
    "                                                               actions_in_flat)))\n",
    "\n",
    "      with tf.variable_scope('activities_hidden'):\n",
    "        boxes_states_pooled = tf.reduce_max(boxes_states, [2])\n",
    "        boxes_states_pooled_flat = tf.reshape(boxes_states_pooled, [-1, NFH])\n",
    "\n",
    "        # TODO: we should be able to\n",
    "        activities_logits = slim.fully_connected(boxes_states_pooled_flat,\n",
    "                                                 c.num_activities,\n",
    "                                                 None)\n",
    "        activities_labels = tf.argmax(activities_logits, 1)\n",
    "        activities_accuracy = tf.reduce_mean(tf.to_float(tf.equal(tf.to_int32(activities_labels),\n",
    "                                                                  activities_in_flat)))\n",
    "        activities_preds = slim.softmax(activities_logits, 'preds')\n",
    "        activities_in_one_hot = slim.one_hot_encoding(activities_in_flat, c.num_activities)\n",
    "        activities_loss = - tf.reduce_mean(activities_in_one_hot * tf.log(activities_preds + EPS))\n",
    "\n",
    "        activities_avg_preds = tf.reduce_mean(tf.reshape(activities_preds, [B,T,c.num_activities]), [1])\n",
    "        activities_avg_labels = tf.to_int32(tf.argmax(activities_avg_preds, 1))\n",
    "\n",
    "        activities_avg_accuracy = tf.reduce_mean(tf.to_float(tf.equal(activities_avg_labels,\n",
    "                                                                      activities_in[:,5])))\n",
    "\n",
    "    sequence_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'ActNet/sequence')\n",
    "\n",
    "  with tf.variable_scope('train'):\n",
    "    global_step = tf.train.create_global_step()\n",
    "    learning_rate = c.train_learning_rate\n",
    "    total_loss = activities_loss + c.actions_loss_weight * actions_loss\n",
    "    train_op = slim.optimize_loss(total_loss,\n",
    "                                  global_step,\n",
    "                                  learning_rate,\n",
    "                                  tf.train.AdamOptimizer)\n",
    "  train_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'train')\n",
    "print('finished building graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/activity/volleyball/stage-b-temporal-actions-2048-5x5-weight-0.5-match-hidden-soft.ckpt\n",
      "(3, 10, 720, 1280, 3)\n",
      "(3, 10, 720, 1280, 3)\n",
      "(3, 10, 720, 1280, 3)\n",
      "(3, 10, 720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "## testing\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.log_device_placement = True\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.allow_soft_placement = True\n",
    "\n",
    "np.random.seed(c.train_random_seed) #can use same random seed as for training\n",
    "tf.set_random_seed(c.train_random_seed)\n",
    "num_iter = int(math.ceil(len(test_frames) / c.batch_size))\n",
    "\n",
    "with tf.Session(config=tf_config) as sess:\n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(sess, c.out_model_path)\n",
    "  \"\"\"\n",
    "  ckpt = tf.train.get_checkpoint_state(c.ckpt_dir)\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    # Restores from checkpoint\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    # Assuming model_checkpoint_path looks something like:\n",
    "    #   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "    # extract global_step from it.\n",
    "    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "  else:\n",
    "    print('No checkpoint file found')\n",
    "    \"\"\"\n",
    "    \n",
    "  #print('loading pre-trained model...')\n",
    "  \n",
    "  #restorer = tf.train.Saver(inception_vars + shared_vars)\n",
    "  #restorer.restore(sess, c.src_model_path)\n",
    "  #print('done!')\n",
    "\n",
    "  #print('initializing the variables...')\n",
    "  #sess.run(tf.initialize_variables(sequence_vars +\n",
    "  #                                 train_vars))\n",
    "  #print('done!')\n",
    "\n",
    "  #saver = tf.train.Saver()\n",
    "\n",
    "  fetches = [\n",
    "    actions_accuracy,\n",
    "    actions_preds,\n",
    "    activities_accuracy,\n",
    "    activities_preds,\n",
    "  ]\n",
    "\n",
    "  evaluations_store = []\n",
    "  for step in range(1, num_iter):\n",
    "    p = range(c.batch_size*(step-1),c.batch_size*step)\n",
    "    batch_frames = sum([_frames_around(test_frames[i],\n",
    "                                       c.num_before,\n",
    "                                       c.num_after)\n",
    "                        for i in p], [])\n",
    "    batch = load_samples_sequence(all_anns, all_tracks, c.images_path, batch_frames)\n",
    "    batch = [b.reshape((c.batch_size,c.num_frames) + b.shape[1:]) for b in batch]\n",
    "\n",
    "    print(batch[0].shape)\n",
    "    feed_dict = {\n",
    "      images_in : batch[0],\n",
    "      activities_in : batch[1],\n",
    "      boxes_in : batch[2],\n",
    "      boxes_idx_in : batch[3],\n",
    "      actions_in : batch[4],\n",
    "      dropout_keep_prob_in : c.train_dropout_prob,\n",
    "    }\n",
    "\n",
    "    outputs = sess.run(fetches, feed_dict)\n",
    "    evaluations_store.append(outputs)\n",
    "    if step % 5 == 0:\n",
    "      ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "      print('%s step:%d ' % (ts, outputs[-1]) +\n",
    "            'actv.A: %.4f ' % (outputs[2]) +\n",
    "            'actn.A: %.4f ' % (outputs[0]))\n",
    "  print(mean(evaluations_store))\n",
    "\n",
    "    \n",
    "  print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
